{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linearities\n",
    "### Rectified Linear Unit\n",
    "* 0 when x < 0\n",
    "* linear when x > 0\n",
    "* derivative is 0 when x < 0, and a constant when x > 0\n",
    "\n",
    "### Neural Network\n",
    "Take logistic classifier\n",
    "\n",
    "x -> Weight Matrix * Bias Matrix -> outputs -> softmax sum to 1 -> Prediction\n",
    "\n",
    "x -> Weight Matrix * Bias Matrix -> ReLU's -> Weight Matrix * Bias Matrix -> outputs -> softmax to 1 -> Prediction\n",
    "\n",
    "* Note that increasing the size of the ReLU layer is not particularly good at making the model better. but chaining them will!\n",
    "### Chain Rule\n",
    "h(x) = (sin(x))^2\n",
    "h'(x) = dh/dx = 2(sin(x)) * cos(x)\n",
    "Derive the outter guy and then derive the inner guy and multiply them\n",
    "\n",
    "Output of one layer is the input of another L = l1(l2(l3(l4))), so the derivative of L = l1' * l2' * l3' * l4. Amazing!\n",
    "\n",
    "This chaining can be decomposed into a very simple data pipeline!\n",
    "\n",
    "### Back Prop\n",
    "You start with forward pass, get the error, and apply that error backward through each layer's derivative (which chain as long as your network is composed of simple functions) and change the weights based on the learning rate.\n",
    "\n",
    "Back Prop takes typically TWICE the memory and compute. Keep that in mind.\n",
    "\n",
    "### Training a Deep Network\n",
    "Increasing the number of hidden layers if typically a good strategy because nn's are naturally good at abstracting hierarchical information for instance in an image, contrasts < features < shapes < objects.\n",
    "\n",
    "### Regularization\n",
    "Deep models are ony good when we have a lot of data. **Skinny Jeans** they look great but hard to get into. So we use neural nets that are too big and then try to prevent them from overfitting.\n",
    "\n",
    "* Early terminating: stop as soon as your performance stops increasing. This is a great way to prevent overfitting.\n",
    "* Regularization: **Stretch Pants** fit well but flexible. L2 regularization is a hyperparameter that penalizes large weights. L2 is added to the norm of the weight mutiplied by a small constant. L' = L + Beta * Norm(Weight)\n",
    "* Dropout: At a random percentage just set activations to 0 for every example you train your network on. Your network cannot rely on any activation to be present, because they may be squashed. Forces your network to be **redundant** so some of the information remains. this strenghtens the \"neurons\". It is kind of faking an ensemble of networks, since so many parts of the networks are forced to learn representations. (if dropout doesnt work, you should use a bigger network!)\n",
    "\n",
    "### Batching\n",
    "* Much faster\n",
    "* Only trains a subset of the dataset at a time.\n",
    "* If your dataset does not fit in memory!\n",
    "* The network gets updated more frequently, so it learns better. Instead of once per whole dataset\n",
    "* Be careful of minibatch, if the batch is not representative enough, it will do gradient descent for unrepresentative learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tftut",
   "language": "python",
   "name": "tftut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
