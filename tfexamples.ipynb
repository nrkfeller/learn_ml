{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfexamples.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nrkfeller/learn_ml/blob/master/tfexamples.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "NXHg0zlFLzJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1y3DRKfL2d3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM for univariate time series"
      ]
    },
    {
      "metadata": {
        "id": "UCTUQDgrL7zk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv('monthly-milk-production.csv', index_col='Month')\n",
        "\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "train_set = df.head(150)\n",
        "test_set = df.tail(18)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_scaled = scaler.fit_transform(train_set)\n",
        "test_scaled = scaler.fit_transform(test_set)\n",
        "\n",
        "def next_batch(training_data,batch_size,steps):\n",
        "    rand_start = np.random.randint(0,len(training_data)-steps) \n",
        "    y_batch = np.array(training_data[rand_start:rand_start+steps+1]).reshape(1,steps+1)\n",
        "    return y_batch[:, :-1].reshape(-1, steps, 1), y_batch[:, 1:].reshape(-1, steps, 1) \n",
        "\n",
        "num_inputs = 1\n",
        "num_time_steps = 12\n",
        "num_neurons = 100\n",
        "num_outputs = 1\n",
        "learning_rate = 0.01\n",
        "num_train_iterations = 8000\n",
        "batch_size = 1\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])\n",
        "\n",
        "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
        "    tf.contrib.rnn.GRUCell(num_units=num_neurons, activation=tf.nn.relu),\n",
        "    output_size=num_outputs) \n",
        "\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for iteration in range(num_train_iterations):\n",
        "        X_batch, y_batch = next_batch(train_scaled, batch_size, num_time_steps)\n",
        "        sess.run(train, feed_dict={X:X_batch, y:y_batch})\n",
        "        if iteration % 100 == 0:\n",
        "            mse = loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
        "            print (\"{} \\t MSE: {}\".format(iteration, mse))\n",
        "    saver.save(sess, './ex_time_series_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9LyA4akDN1v5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./ex_time_series_model\")\n",
        "    train_seed = list(train_scaled[-12:])\n",
        "    for iteration in range(12):\n",
        "        X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
        "        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
        "        train_seed.append(y_pred[0, -1, 0])\n",
        "        \n",
        "results = scaler.inverse_transform(np.array(train_seed[12:]).reshape(12,1))\n",
        "test_set = test_set[:12]\n",
        "test_set['Generated'] = results\n",
        "\n",
        "plt.plot(test_set['Generated'])\n",
        "plt.plot(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fm3iIIFzOA52",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word to Vec"
      ]
    },
    {
      "metadata": {
        "id": "q9mQIdI6ODhR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import errno\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange  \n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "\n",
        "data_dir = 'word2vec_data/words'\n",
        "data_url = 'http://matthoney.net/dc/text8.zip'\n",
        "\n",
        "def fetch_words_data(url=data_url, words_data=data_dir):\n",
        "    os.makedirs(words_data, exist_ok=True)\n",
        "    zip_path = os.path.join(words_data, 'words.zip')\n",
        "    \n",
        "    if not os.path.exists(zip_path):\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "        \n",
        "    with zipfile.ZipFile(zip_path) as f:\n",
        "        data = f.read(f.namelist()[0])\n",
        "        \n",
        "    return data.decode('ascii').split()\n",
        "\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    if data_index + span > len(data):\n",
        "        data_index = 0\n",
        "    buffer.extend(data[data_index:data_index + span])\n",
        "    data_index += span\n",
        "    for i in range(batch_size // num_skips):\n",
        "        target = skip_window  # target label at the center of the buffer\n",
        "        targets_to_avoid = [skip_window]\n",
        "        for j in range(num_skips):\n",
        "            while target in targets_to_avoid:\n",
        "                target = random.randint(0, span - 1)\n",
        "            targets_to_avoid.append(target)\n",
        "            batch[i * num_skips + j] = buffer[skip_window]\n",
        "            labels[i * num_skips + j, 0] = buffer[target]\n",
        "    if data_index == len(data):\n",
        "        buffer[:] = data[:span]\n",
        "        data_index = span\n",
        "    else:\n",
        "        buffer.append(data[data_index])\n",
        "        data_index += 1\n",
        "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, labels\n",
        "\n",
        "def create_counts(vocab_size=50000):\n",
        "    vocab = [] + Counter(words).most_common(vocab_size)\n",
        "    vocab = np.array([word for word, _ in vocab])\n",
        "    dictionary = {word:code for code, word in enumerate(vocab)}\n",
        "    data = np.array([dictionary.get(word, 0) for word in words])\n",
        "    return data, vocab\n",
        "  \n",
        "  \n",
        "words = fetch_words_data()\n",
        "  \n",
        "data, vocabulary = create_counts()\n",
        "\n",
        "batch_size = 128\n",
        "embedding_size = 150\n",
        "skip_window = 1\n",
        "num_skips = 2\n",
        "\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "num_sampled = 64\n",
        "learning_rate = 0.01\n",
        "vocabulary_size = 50000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
        "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
        "embeddings = tf.Variable(init_embeds)\n",
        "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "\n",
        "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0/np.sqrt(embedding_size)))\n",
        "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, num_sampled, vocabulary_size))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 1.0)\n",
        "trainer = optimizer.minimize(loss)\n",
        "\n",
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
        "normalized_embeddings = embeddings / norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "\n",
        "data_index = 0\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "num_steps = 5000\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    average_loss = 0\n",
        "    for step in range(num_steps):\n",
        "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
        "        feed_dict = {train_inputs:batch_inputs, train_labels:batch_labels}\n",
        "        _, loss_val = sess.run([trainer, loss], feed_dict=feed_dict)\n",
        "        average_loss += loss_val\n",
        "        \n",
        "        if step % 1000 == 0:\n",
        "            if step > 0:\n",
        "                average_loss = average_loss/1000\n",
        "            print(\"average loss at step: {} is {}\".format(step, average_loss))\n",
        "            average_loss = 0\n",
        "        final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k8Hs8s7FZCTm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
        "\n",
        "plot_only = 5000\n",
        "low_dim_embed = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
        "\n",
        "labels = [vocabulary[i] for i in range(plot_only)]\n",
        "\n",
        "def plot_with_labels(low_dim_embs, labels):\n",
        "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
        "    plt.figure(figsize=(18,18))\n",
        "    for i , label in enumerate(labels):\n",
        "        x, y = low_dim_embs[i,:]\n",
        "        plt.scatter(x,y)\n",
        "        plt.annotate(labels,\n",
        "                     xy=(x, y),\n",
        "                     xytext=(5,2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "        \n",
        "plot_with_labels(low_dim_embed, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TP6RGqHlZXUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# "
      ]
    }
  ]
}