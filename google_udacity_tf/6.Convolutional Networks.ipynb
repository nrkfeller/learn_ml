{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs\n",
    "* color doesnt matter, then dont use colored images\n",
    "* translation invariance, doesnt matter where the cat is\n",
    "* Statistical Invariance: things that don't change based on time and space are everywhere and should be treated the same\n",
    "\n",
    "### Convolutions\n",
    "Neural networks that share their parameters accross space\n",
    "1. Slide a little NN accross the surface without changing the weight\n",
    "2. The layer it has created has a different width, height and depth!\n",
    "\n",
    "If the kernel was the size of the entire picture, it would just be a regular neural net!\n",
    "\n",
    "Each convolution progressively squeeze the spatial dimensions. at the end all your spatial representation is out, and you only have features!\n",
    "\n",
    "### ReLU\n",
    "performs better than the logistic sigmoid\n",
    "\n",
    "### Padding\n",
    "* valid padding is when you dont go past\n",
    "* same padding is when you go past such that the next layer is the same size\n",
    "\n",
    "### Pooling\n",
    "Instead of increasing th stride, which loose some information. after a convolution we pool the information into a new layer.\n",
    "1. max pooling: take a portion of the feature map and keep the largest response. Oftern more accurate, doesnt lead to overfitting, but more expensive computationally. conv-pool-conv-pool-full-full-class (lenet)\n",
    "2. average pooling: just take an average over a window, like a blurring\n",
    "\n",
    "### 1x1 convolutions\n",
    "can increase or decrease the dimensionality of your convolution layers\n",
    "\n",
    "### Inception Module\n",
    "At each layer choose to have pooling, convolutions and the size. Instead of having a single choice, just do all of them and concatenate the outputs. This performs very well.\n",
    "\n",
    "### dense vs sparse\n",
    "dense is fully connected, sparse is not\n",
    "\n",
    "### Conclusions\n",
    "With a general framework like neural nets and all the associated building blocks, we can be quite creative about how we stich things together.\n",
    "\n",
    "### Quiz\n",
    "image = 28 * 28, kernel 3 * 3\n",
    "\n",
    "|   padding| stride  |  Width |  Height |  Depth |\n",
    "|---|---|---|---|---|\n",
    "|  Same | 1  |  28 | 28  |   8|\n",
    "| Valid  |  1 |26   | 26  |  8 |\n",
    "| Valid  |  2 |  13 |  13 |  8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "\n",
    "\n",
    "LeNet: Yann LeCun 1990 Convolutional neural network that had some very good results. Used for reading information on mail\n",
    "\n",
    "AlexNet: Alex Krizhevsky. BIG DEAL. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. Inception! creazy good results\n",
    "\n",
    "Since then convnets is the way to go for image classification!\n",
    "\n",
    "Talk about notMNIST and benchmarking\n",
    "ImageNet : large public database\n",
    "\n",
    "On images so its x by x by 3\n",
    "PAGE 3\n",
    "* input - conv - relu - pool - f\n",
    "1. CONV: dot product between neuron weights and local region. Thicker region, depending on how big the kernel (heavylifting). matches and scores the layers and weigfhts (page 13). how well does this piece match what I am looking for? (page 10). \n",
    "2. ReLU: function that is applied to convlayer. elementwise activation function, keeps output the same. introduces non linearity (no hyper parameters). No vanishing gradient problem, efficient to compute, scale invariance!\n",
    "3. POOL: introduces downsampling. makes the output smaller. Takes a square and uses the largest or average - max_pool. identifies heirarchical information\n",
    "4. FC: introduces score: say for letter is 0-25 and takes the one with highest value - softmax\n",
    "* this is not exhastive but they work very well! Pretty much what works on imagenet\n",
    "\n",
    "Check tensorflow code\n",
    "\n",
    "### Hyperparameters\n",
    "1. dropout\n",
    "2. depth\n",
    "3. size of kernel\n",
    "4. number of kernels - inception\n",
    "5. variation of pooling and conv+relu\n",
    "6. batch size\n",
    "7. padding\n",
    "8. number of hidden nodes\n",
    "9. learning rate\n",
    "10. number of iterations\n",
    "11. momentum\n",
    "12. weight decay\n",
    "13. stoppage\n",
    "* these are not exhaustie, just the ones i could come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tftut",
   "language": "python",
   "name": "tftut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
