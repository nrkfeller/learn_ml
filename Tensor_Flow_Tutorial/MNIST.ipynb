{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "\n",
    "# how much should we take each learned data\n",
    "# too large, might skip optimal, to small takes too much time/data/iterations\n",
    "learning_rate = 0.01\n",
    "\n",
    "# \n",
    "training_iteration = 30\n",
    "\n",
    "#\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create main model\n",
    "# model = dataflow graph\n",
    "# nodes in model = nodes , nodes = units of computation\n",
    "\n",
    "# each node takes in a tensor and outputs a tensor\n",
    "# tensor is the data rep, multi dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input, one for each pixel\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "\n",
    "# ouput, one for each class\n",
    "y = tf.placeholder('float', [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "# Weight are updated during training - graph weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# Bias shifts the regression line\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helps us visualize the data\n",
    "with tf.name_scope(\"Wx_b\") as scope:\n",
    "    # linear regression model\n",
    "    model = tf.nn.softmax(tf.matmul(x, W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary operations to vizualize and collect data\n",
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# help minimize error during training\n",
    "# here we use cross entropy\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    \n",
    "    # scalar summary to monitor cost function\n",
    "    tf.summary.scalar(\"cost_function\", cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\") as scope:\n",
    "    # gradient descent is paced by the learning rate\n",
    "    # and minimized with cost_function\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "iteration 0001 cost 29.860465115\n",
      "iteration 0003 cost 21.126813491\n",
      "iteration 0005 cost 20.324054164\n",
      "iteration 0007 cost 19.629978932\n",
      "iteration 0009 cost 19.336096870\n",
      "iteration 0011 cost 19.083457223\n",
      "iteration 0013 cost 18.995934640\n",
      "iteration 0015 cost 18.749809804\n",
      "iteration 0017 cost 18.580422345\n",
      "iteration 0019 cost 18.518634330\n",
      "iteration 0021 cost 18.481242117\n",
      "iteration 0023 cost 18.462068937\n",
      "iteration 0025 cost 18.250765198\n",
      "iteration 0027 cost 18.113451968\n",
      "iteration 0029 cost 18.128990116\n",
      "training complete\n",
      "accuracy 0.9218\n"
     ]
    }
   ],
   "source": [
    "# Lauching graph session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter('.', graph_def=sess.graph)\n",
    "    \n",
    "    # training cycle\n",
    "    for iteration in range(training_iteration):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # loop over batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # fir training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y:batch_ys})\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs,y: batch_ys})/total_batch\n",
    "\n",
    "            # write to summary for tf board\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y:batch_ys})\n",
    "            summary_writer.add_summary(summary_str, iteration*total_batch+i)\n",
    "\n",
    "        # pritns logs\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"iteration\", '%04d' % (iteration+1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "    print(\"training complete\")\n",
    "    \n",
    "    # check how model values measure up against test values\n",
    "    predictions = tf.equal(tf.arg_max(model, 1), tf.arg_max(y, 1))\n",
    "    \n",
    "    # mean accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"accuracy\", accuracy.eval({x:mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ tensorboard --logdir=."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftut",
   "language": "python",
   "name": "tftut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
