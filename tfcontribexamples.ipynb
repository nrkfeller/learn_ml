{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfcontribexamples.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nrkfeller/learn_ml/blob/master/tfcontribexamples.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "zi4x-2sScchN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DNNClassifier"
      ]
    },
    {
      "metadata": {
        "id": "41P9m_7Ub-I_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import estimator \n",
        "\n",
        "wine_data = load_wine()\n",
        "\n",
        "feat_data = wine_data['data']\n",
        "labels = wine_data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feat_data,\n",
        "                                                    labels,\n",
        "                                                    test_size=0.3,\n",
        "                                                   random_state=101)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_x_train = scaler.fit_transform(X_train)\n",
        "scaled_x_test = scaler.transform(X_test)\n",
        "\n",
        "feat_cols = [tf.feature_column.numeric_column(\"x\", shape=[13])]\n",
        "\n",
        "deep_model = estimator.DNNClassifier(hidden_units=[13,13,13],\n",
        "                            feature_columns=feat_cols,\n",
        "                            n_classes=3,\n",
        "                            optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01) )\n",
        "\n",
        "input_fn = estimator.inputs.numpy_input_fn(x={'x':scaled_x_train},y=y_train,shuffle=True,batch_size=10,num_epochs=5)\n",
        "\n",
        "deep_model.train(input_fn=input_fn,steps=500)\n",
        "\n",
        "input_fn_eval = estimator.inputs.numpy_input_fn(x={'x':scaled_x_test},shuffle=False)\n",
        "\n",
        "preds = list(deep_model.predict(input_fn=input_fn_eval))\n",
        "\n",
        "predictions = [p['class_ids'][0] for p in preds]\n",
        "\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4ePpfRlcmVB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras"
      ]
    },
    {
      "metadata": {
        "id": "XgLUoVkBcm81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbbbb3f3-4ec7-4d9c-ce8e-c43262203776"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.keras import models\n",
        "from tensorflow.contrib.keras import layers\n",
        "from tensorflow.contrib.keras import losses,optimizers,metrics\n",
        "\n",
        "dnn_keras_model = models.Sequential()\n",
        "dnn_keras_model.add(layers.Dense(units=13,input_dim=13,activation='relu'))\n",
        "dnn_keras_model.add(layers.Dense(units=13,activation='relu'))\n",
        "dnn_keras_model.add(layers.Dense(units=13,activation='relu'))\n",
        "dnn_keras_model.add(layers.Dense(units=3,activation='softmax'))\n",
        "\n",
        "losses.sparse_categorical_crossentropy\n",
        "\n",
        "dnn_keras_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dnn_keras_model.fit(scaled_x_train,y_train,epochs=50, verbose=0) # change verbose=1 to track progress"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f93d6035e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "mQQUlqc2c8mL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "42e2544e-0699-4714-91c1-b2e75b6eecdd"
      },
      "cell_type": "code",
      "source": [
        "predictions = dnn_keras_model.predict_classes(scaled_x_test)\n",
        "print(classification_report(predictions,y_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.95      1.00      0.97        18\n",
            "          1       1.00      0.96      0.98        23\n",
            "          2       1.00      1.00      1.00        13\n",
            "\n",
            "avg / total       0.98      0.98      0.98        54\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FfjLUHVodCcB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Layers API"
      ]
    },
    {
      "metadata": {
        "id": "GmETp9oOdBO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "105b6520-52d4-4654-d75d-8387dcca5ea2"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import fully_connected\n",
        "\n",
        "wine_data = load_wine()\n",
        "feat_data = wine_data['data']\n",
        "labels = wine_data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feat_data,\n",
        "                                                    labels,\n",
        "                                                    test_size=0.3,\n",
        "                                                   random_state=101)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_x_train = scaler.fit_transform(X_train)\n",
        "scaled_x_test = scaler.transform(X_test)\n",
        "# ONE HOT ENCODED\n",
        "onehot_y_train = pd.get_dummies(y_train).as_matrix()\n",
        "one_hot_y_test = pd.get_dummies(y_test).as_matrix()\n",
        "\n",
        "num_feat = 13\n",
        "num_hidden1 = 13\n",
        "num_hidden2 = 13\n",
        "num_outputs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "X = tf.placeholder(tf.float32,shape=[None,num_feat])\n",
        "y_true = tf.placeholder(tf.float32,shape=[None,3])\n",
        "\n",
        "actf = tf.nn.relu\n",
        "\n",
        "hidden1 = fully_connected(X,num_hidden1,activation_fn=actf)\n",
        "hidden2 = fully_connected(hidden1,num_hidden2,activation_fn=actf)\n",
        "output = fully_connected(hidden2,num_outputs)\n",
        "\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels=y_true, logits=output)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "training_steps = 1000\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(training_steps):\n",
        "        sess.run(train,feed_dict={X:scaled_x_train,y_true:onehot_y_train})\n",
        "        \n",
        "    # Get Predictions\n",
        "    logits = output.eval(feed_dict={X:scaled_x_test})\n",
        "    \n",
        "    preds = tf.argmax(logits,axis=1)\n",
        "    \n",
        "    results = preds.eval()\n",
        "    \n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "print(classification_report(results,y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      0.95      0.97        20\n",
            "          1       0.95      1.00      0.98        21\n",
            "          2       1.00      1.00      1.00        13\n",
            "\n",
            "avg / total       0.98      0.98      0.98        54\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6I0pIXHwg4Ia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}